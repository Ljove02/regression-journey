# ğŸš€ Regression Journey

A comprehensive exploration of machine learning regression techniques through two real-world case studies:

Predicting student academic performance and SpaceX Falcon 9 landing outcomes.

## ğŸ“Š Projects Overview

### 1. ğŸ“ Student Performance Prediction (Linear Regression)

Predicting student performance index using academic and lifestyle factors.

**Key Features:**

- **Algorithm**: Linear Regression with Polynomial Features (degree 2)
- **Performance**: 98.87% RÂ² accuracy
- **Features**: Study hours, previous scores, extracurricular activities, sleep patterns
- **Techniques**: Feature standardization, polynomial expansion, interactive hyperparameter tuning

### 2. ğŸš€ Falcon 9 Landing Prediction (Logistic Regression)

Machine learning model to predict SpaceX Falcon 9 first-stage landing success.

**Key Features:**

- **Algorithm**: Logistic Regression with Polynomial Features (degree 3) + L2 Regularization
- **Performance**: 89% Precision/Recall/F1-Score, AUC = 0.84
- **Features**: Payload mass, orbit type, technical specs (Block, GridFins, Legs), flight history
- **Techniques**: One-hot encoding, feature engineering, ROC analysis

## ğŸ› ï¸ Technical Implementation

- **Custom ML Library**: Built using custom `kockice` library for deeper algorithmic understanding
- **Interactive Notebooks**: Marimo notebooks with real-time parameter tuning
- **From Scratch**: Manual implementation of gradient descent, loss functions, and evaluation metrics
- **Professional Workflow**: Complete ML pipeline from EDA to model evaluation

## ğŸ“ˆ Results Summary

| Project             | Algorithm           | Accuracy     | Key Insight                                                                    |
| ------------------- | ------------------- | ------------ | ------------------------------------------------------------------------------ |
| Student Performance | Linear Regression   | 98.87% RÂ²    | Linear relationships dominate - polynomial features didn't improve performance |
| Falcon 9 Landings   | Logistic Regression | 89% F1-Score | GridFins (0.64) and Legs (0.67) are strongest predictors of landing success    |

## ğŸ”§ Setup & Usage

### Prerequisites

```bash
pip install marimo pandas numpy matplotlib seaborn plotly scikit-learn kagglehub
```

### Running the Notebooks

```bash
# Student Performance Analysis
marimo run student_performance_linear_regression.py

# Falcon 9 Landing Prediction
marimo run falcon9_logistic_regression.py
```

### ğŸ“‹ Quick Preview on GitHub

For a quick overview without installation, you can **view the Jupyter notebook versions directly on GitHub**:

- **[ğŸ“ Student Performance Analysis](student_performance.ipynb)** - Browse the complete analysis with visualizations
- **[ğŸš€ Falcon 9 Landing Prediction](falcon9.ipynb)** - Explore the SpaceX landing model with charts and metrics

*Note: GitHub's notebook viewer renders all outputs, plots, and markdown for easy reading without running code.*

## ğŸ“ Project Structure

```
regression-journey/
â”œâ”€â”€ falcon9_logistic_regression.py       # SpaceX landing prediction (Marimo)
â”œâ”€â”€ student_performance_linear_regression.py  # Student performance analysis (Marimo)
â”œâ”€â”€ falcon9.ipynb                        # SpaceX analysis (Jupyter - GitHub viewable)
â”œâ”€â”€ student_performance.ipynb           # Student performance (Jupyter - GitHub viewable)
â”œâ”€â”€ data/                               # Dataset files
â”œâ”€â”€ images/                             # Visualizations and assets
â”œâ”€â”€ kockice.py                          # Custom ML library
â””â”€â”€ README.md                           # Project documentation
```

## ğŸ§  Key Learning Outcomes

- **Feature Engineering**: Polynomial expansion, one-hot encoding, standardization
- **Model Evaluation**: ROC curves, confusion matrices, precision/recall analysis
- **Regularization**: L2 penalty to prevent overfitting
- **Domain Knowledge**: Understanding real-world constraints (SpaceX landing procedures)
- **Interactive ML**: Building responsive parameter tuning interfaces

## ğŸ“Š Visualizations

Both projects include comprehensive visualizations:

- Correlation matrices and heatmaps
- Loss curves and convergence analysis
- ROC curves and performance metrics
- Interactive scatter plots and distribution charts

## ğŸ¯ Technical Highlights

- **Custom implementations** of gradient descent, BCE loss, MSE loss
- **Stratified sampling** for balanced train/test splits
- **Polynomial feature engineering** while avoiding redundant OHE combinations
- **Professional evaluation** using multiple metrics beyond accuracy

## ğŸ”® Future Enhancements

- Cross-validation implementation
- Comparison with ensemble methods (Random Forest, XGBoost)
- Feature importance analysis
- Model deployment as REST API
- Deep learning approaches for comparison

---

_Built with passion for machine learning and space exploration_ ğŸŒŸ
